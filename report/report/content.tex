%--------------------------------------------------------------------------
% COMP61011, project
% Edward John
% University of Manchester
%--------------------------------------------------------------------------
\definecolor{light-gray}{gray}{0.9}
\newcommand{\todo}[1]{
    \colorbox{light-gray}{
        \parbox{\linewidth}{
            \textbf{TODO} #1
        }
    }
}

\nocite{github}

%--------------------------------------------------------------------------
% Abstract
%--------------------------------------------------------------------------
\begin{abstract} 
Issue tracking in software development usually requires the submission of text explaining the issue. Issues tend to have labels
associated with them to help categorise them so that they can be more easily searched for. Examples of labels could include
\textit{bug}, \textit{suggestion} or \textit{question} and may not be mutually exclusive. GitHub is a website that provides
issue tracking to public code repositories. This allows any GitHub user to submit issues where repository collaborators can then
label them.

This projects explores how common labels in particular (\textit{bug}, \textit{suggestion} and \textit{question}) can be
predicted for GitHub issues by first extracting features from the issue text in the form of words and phrases. This can then be
processed using machine learning techniques such as the k-Nearest Neighbour classifier and finally measuring and comparing the
performance over the trial and error and mutual information feature selection techniques to improve the accuracy of the
classifier.
\end{abstract}

%--------------------------------------------------------------------------
% Introduction
%--------------------------------------------------------------------------
\section{Introduction}
GitHub is a website of the US company GitHub, Inc. that offers online hosting of code repositories and support for project team
collaboration. One of the provided features for each hosted `Git' repository is an issue tracker. The issue tracker allows users
to submit issues. Types of issues can include; bug reports, feature suggestions, improvements or questions. When an issue is
submitted, collaborators of the repository can add labels to it to help categorise the issue. Labels may not be mutually
exclusive, therefore an issue could have the labels; \textit{bug} and \textit{high priority}. Because labels have to be manually
assigned by collaborators, issues may not be labelled for some time depending on the availability and number of collaborators.
As a result filtering and sorting may not select or collate all the related issues of a particular type, and collaborators have
to be relied upon to be diligent in keeping up to date with their labelling new issues.

Similar to spam e-mail classifiers, an issue can be classified by analysing the words and phrases in the body text of an issue.
This is known as the bag-of-words model \cite{wikipedia:bagofwords} where a document or block of text is split up into a
multi-set of its words / tuples of words, often disregarding punctuation and order but maintaining existence or frequency. The
words and phrases are used as features for the classifier. To improve the accuracy of the classifier and reduce the search
space, only the significant features need to be analysed, this is known as `feature selection'. This will likely include stop
words which are found in nearly all bodies of text and therefore only acts as unwanted noise.

Feature selection can be used to improve the classifiers individually for the labels \textit{bug}, \textit{suggestion} and
\textit{question} by giving each feature a different weight counting towards each label classification.

%--------------------------------------------------------------------------
% Background
%--------------------------------------------------------------------------
\section{Background}
\subsection{Information extraction}
Before any experiments could be made on predicting labels for issues, information extraction was performed by writing an
application that downloaded all the issues for a given GitHub repository and extracted features and labels from each issue. The
application used the Octokit \cite{octokit} library provided by GitHub to download the issues using their application
programming interface (API). These features consisted of words and pairs of words (phrases) from within the body text of an
issue. The labels were individual non-mutual exclusive flags for whether the issue had been labelled as \textit{bug},
\textit{suggestion} or \textit{question}. This stage is known as `text mining'.

\subsection{Word / phrase frequencies}
\label{sec:background_wordfreq}
As a preliminary form of feature selection, a word and phrase frequency analysis was performed on all the issues for three
different GitHub repositories (``jquery-handsontable'', ``TypeScript'' and ``OpenRCT2''). The number of instances of all single
words, pairs of words and triples of words were counted for each issue that was labelled \textit{bug}, \textit{suggestion} and
\textit{question} respectively. This was performed for each of the three repositories. All words or phrases that only occurred
once in each individual repository were removed, the remaining words and phrases were then further filtered leaving just the
words and phrases that existed with in all three repositories. This was to eliminate specific terminology only found for a
particular repository, e.g. \textit{JavaScript} for the ``TypeScript'' repository and therefore leaving a more generalised set
of words and phrases that could be used for any repository. Stop words were also removed as they would act as noise during the
training stage, due to their abundance in most literature. A list of stop words provided by ranks.nl \cite{ranks.nl} was used.
tf-idf is an equation that can be used to measure how important a word or phrase is based on its global frequency over all
documents and its frequency for a particular document. This could have been used to further reduce the list of words to include
as features, however removing a set of known general stop words was considered to be enough to reduce the set as feature
selection based on classifier performance will also take place.

\subsection{Feature selection}
Once initial tests with all the features have been run, the number of features can be narrowed down to reduce the size of the
problem and also reduce the error rate by removing irrelevant or other noisy features. The `trial and error',
`mutual information' and `conditional mutual information' techniques were utilised as they are known to work well on discrete
features, unlike `Pearson's correlation coefficient' which is more fitted for continuous data.

%--------------------------------------------------------------------------
% Experiments
%--------------------------------------------------------------------------
\section{Experiments}
\subsection{k-Nearest Neighbour classifier}
The k-Nearest Neighbour (k-NN) classifier requires no learning algorithm to produce a model. Instead it classifies by testing
against every example within the training data. k-NN classifiers can provide accurate results but are however very
computationally expensive due the the $O(nd)$ running time where $n$ is the number of examples and $d$ is the number of
features. It classifies an example by measuring the distance between the testing example and every training example using a
distance function, for example `Euclidean' or `Hamming'. It then chooses the most common class in the nearest $k$ examples.
There are many other techniques that could be used to optimise the algorithm but none were considered relevant to this project's
objective.

Due to the binary nature of whether a word or phrase exists or not in the body text of an issue, a distance function such as
Euclidean would not be appropriate. Instead the distance can be calculated using the Hamming formula as shown in
Equation~\ref{eq:hamming}. This is the total number of features that are different between two examples. Therefore two identical
examples will produce a distance of zero and will increase as more words and phrases are present in one example and not the
other.
\begin{equation}
    \label{eq:hamming}
    \mathrm{hamming}(x,x') = \sum^d_{j=1} \delta (x_{j} \neq x'_{j})
\end{equation}
where $\delta$ is the indicator function which returns 0 if the word or phrase is present in both examples or neither examples
and 1 if the word or phrase is only present in one example but not the other.

\subsection{Measuring the performance}
Before attempting to improve the classifier using feature selection, a method of measuring the performance reliably is required
so that any improvement or degradation is shown with the performance values before and after manipulation of the classifier. The
performance is measured based on the number of correct predictions made compared with the number of incorrect predictions made,
known as the error rate. Splitting up the examples (obtained from the application that was written to extract issues from
GitHub) into two parts allows one part to be used for training and the other part to be used for testing. Since the correct
labels are known, the predicted labels given to the testing examples can be compared with the correct labels. The number of
incorrect predictions made over the number of total predictions is the error rate. It was noted that some examples in the data
could have been incorrectly labelled. The labelling can also be affected by one person's opinion which may be different from
another person's.

The process can be performed multiple times where the data is shuffled before each case to reduce the uncertainty and obtain
standard deviation values which show the variation or the amount of error.

\subsubsection{Cross validation}
Cross validation involves splitting the examples into multiple folds so that each fold can be used for testing and the rest for
training. The final error rates can be averaged to generalise the classifier's performance as it takes into account
multiple sets of training data and testing data in isolation.

This can be further improved by a technique known as Leave One Out Cross Validation (LOOCV) which involves performing cross
validation on all but one of the folds the data is split into. The folds that give the lowest error rate when used as training
data is then used as the training data for the fold that was not used in the cross validation. The error rate produced by this
final test is then used as the measure of performance.

\subsubsection{Initial results}
\label{sec:initial_results}
Without any feature selection, an average error rate across all labels using cross validation was 26\%. The error rate for just
the \textbf{bug} label was significantly higher at 36\%. This was found across a range of different $k$ values between 1 and 53.
Figure~\ref{fig:raw_errors} shows the error rates for each label over the value $k$. The uncertainty was quite low with
standard deviation values below 0.02, this can be seen from the error bars.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{charts/cmim_errors.pdf}
    \caption{Raw error rate results prior to feature selection.}
    \label{fig:raw_errors}
\end{figure}

\subsection{Feature selection}
The k-NN classifier was modified to apply feature selection. This was achieved by allowing each feature to have an independent
weight which affected the distance between two examples. Each feature selection technique assigns a weight to each feature where
$w_f = 0$ is a feature that is ignored. This distance function is revised in equation~\ref{eq:hamming_weights}. This will
produce higher distances for features that correlate more. If the weight is 0, the feature is implicitly ignored.
\begin{equation}
    \label{eq:hamming_weights}
    \mathrm{hamming}(x,x') = \sum^d_{j=1} w_{j} \delta (x_{j} \neq x'_{j})
\end{equation}
where $w_{j}$ is the weight of the feature.

\subsubsection{Trial and error method}
The first method investigated was a single pass trial and error method. This is a wrapper method as it is based on achieving the
lowest error rate for a particular model. The method is a greedy algorithm where each feature is selected randomly and given a
weight of zero. The classifier is then tested to see if the accuracy improved or not. If the accuracy improved, the feature is
concluded as noise, if the accuracy did not change by any significant amount, the feature was concluded to be irrelevant. If the
accuracy dropped, the feature was concluded as significant.

This could be done repeatedly until all combinations of features are searched, however that is an intractable problem. Therefore
just a single pass was performed in a random order so the results were more generalised, as the order could dictate the
performance based on multiple sets of features. This means redundancy is not taken into account and could impact the results.

Each feature was initially given a weight of one. An initial classifier test was performed to calculate the error rate for when
all the features are used. The algorithm then randomly selects a feature and sets its weight to zero before running the test
again. If the test returns an error rate that is significantly higher than the best error rate, the feature weight is reset back
to one. Otherwise the best error rate is updated accordingly and the process continues with another random feature until all
features have been searched. The remaining features with a weight of one are significant to keeping the error rate low, the
features with a weight of zero are deemed to be noise. This does not take into account pairs of features or more, which is one
reason why the features are searched in a random order, giving a more generalised final result.
See algorithm~\ref{alg:trialerror}.

\begin{algorithm}
    \caption{Trial and error method}
    \label{alg:trialerror}
    \begin{algorithmic}
        \STATE $w[0..N] \gets 1$
        \STATE $best\_ error \gets \mathrm{test\_ classifier}(w)$
        \FOR {$i \gets \mathrm{shuffle}(1 \dots N)$}
            \STATE $w[i] \gets 0$
            \STATE $error \gets \mathrm{test\_ classifier}(w)$
            \IF {$error < best\_ error$}
                \STATE $best\_error \gets error$
            \ELSIF {$error > best\_ error + tolerance$}
                \STATE $w[i] \gets 1$
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

A tolerance value of 0.02 was used which is the highest standard deviation value seen in the initial error rate results before
feature selection shown in Section~\ref{sec:initial_results}. A $k$ value of 3 and 23 was used against all three labels. The
results are shown in Figure~\ref{fig:trialerror_errors}. The results did not shown any improvement over the original results
before feature selection before Figure~\ref{fig:raw_errors}, it did however significantly reduce the number of features
improving the efficiency, this can be seen in Table~\ref{tbl:trialerror_inc_features}.

\begin{table}[h]
    \centering
    \begin{tabular}{l|c|c}
                   & \multicolumn{2}{l}{Included Features} \\
        \hline
        k          & 3                 & 23                \\
        \hline
        bug        & 68                & 32                \\
        suggestion & 34                & 9                 \\
        question   & 39                & 0                 \\
        \hline
    \end{tabular}
    \caption{Number of included features after trial and error feature selection.}
    \label{tbl:trialerror_inc_features}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{charts/wrapper_errors.pdf}
    \caption{Error rate results after trial and error feature selection.}
    \label{fig:trialerror_errors}
\end{figure}

\subsubsection{Mutual Information}
\label{sec:mutual_informaton}
Entropy shows us the measurement of information for a set of data or in other words, the ``amount of randomness''. Entropy can
be used to measure the amount of information gain for a particular feature. By using equation~\ref{eq:entropy}, where $X$ is the
set of feature values or label values, the amount of information in the probability distribution of the set can be calculated.
\begin{align}
    \label{eq:entropy}
    H(X) = -\sum_{x \in X} p(x) \log p(x)
\end{align}

Because this scenario only contains binary features (non-existent and existent words / phrases) and binary classes
(e.g. bug or not bug), equation~\ref{eq:binary_entropy} can be used instead which is derived from equation~\ref{eq:entropy} but
with the probability and compliment substituted for the summation.
\begin{equation}
    \label{eq:binary_entropy}
    \begin{split}
        H(X) &= -(p(x) \log p(x) + p(x^c)     \log p(x^c)    \\
        H(X) &= -(p(x) \log p(x) + (1 - p(x)) \log(1 - p(x))
    \end{split}
\end{equation}

To determine whether a feature correlates to a particular class such as \textit{bug}, the mutual information can be calculated
using equation~\ref{eq:mutual_information}, where $H(Y)$ is the entropy of the class and $H(Y|X)$ is the entropy of the class
given the feature. This will produce numbers within the range $0 \leq I(X;Y) \leq \min(H(X), H(Y))$. The higher the value, the
more the feature correlates to the class, either when it exists or doesn't exist. Each feature can then have an independent
weight based on the mutual information (MI) for a particular class or by its rank within the set of features. This is a filter
feature selection technique as it uses a proxy measure instead of the error rate to score a subset of
features \cite{wikipedia:featureselection}.
\begin{equation}
    \label{eq:mutual_information}
    I(X;Y) = H(Y) - H(Y|X)
\end{equation}

Figure~\ref{fig:mi_prob} shows the probability, $p(\mathrm{bug}|f)$ and mutual information, $I(f,\mathrm{bug})$ for each feature
where the mutual information is above 0.13 bits. The features are ordered from lowest probability to highest probability showing
the least common words / phrases found in issues labelled \textit{bug} and the most common words / phrases found in issues
labelled \textit{bug}. The closer the probability is to 0.5, the lower the mutual information, therefore the words / phrases
with very low probability and very high probability will have a high weight as they correlate the most to the label
\textit{bug}.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{charts/mi_prob.pdf}
    \caption{Feature probability and mutual information distribution.}
    \label{fig:mi_prob}
\end{figure}

After calculating the mutual information for each feature and using it for the feature's weight that is applied in the k-NN
distance function as shown in Equation~\ref{eq:hamming_weights}. The error rate for the label \textit{bug} lowered to 33\%. The
feature selection technique had surprisingly made little difference in improving the accuracy. The experiment was then slightly
modified again so that any feature with a mutual information of less than 0.1 bits was ignored. This lowered the number of
features from 93 to just 34. The error rate was 36\%, the same accuracy prior to feature selection. This meant that over half of
the features were completely irrelevant for the label \textit{bug}. However when ignoring all features with a mutual information
of less than 0.5 bits, there were only 11 features remaining which produced an error rate of 41\%.

The error rates for \textit{suggestion} and \textit{bug} were consistently 0.22 and 0.19 respectively using the same technique.
It was suspected that a k-NN classifier where $k = 23$ was underfitting the data for \textit{suggestion} and \textit{question}
but not for \textit{bug}. The same tests were performed again with $k = 3$, the results are shown in
Table~\ref{tbl:mi_errors_k3} and Figure~\ref{fig:mi_errors_k3}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c||cc|cc|cc|}
    \hline

    % First row
    & \multicolumn{6}{c|}{error rate ($\sigma$)} \\

    % Second row
    min. &
    \multicolumn{2}{c|}{\textbf{bug}} &
    \multicolumn{2}{c|}{\textbf{suggestion}} &
    \multicolumn{2}{c|}{\textbf{question}}
    
    \\
    \hline

    % Data
    0.0 & 0.32 & (0.00) & 0.24 & (0.01) & 0.25 & (0.01) \\
    0.1 & 0.36 & (0.02) & 0.31 & (0.13) & 0.23 & (0.06) \\
    0.2 & 0.38 & (0.02) & 0.31 & (0.13) & 0.28 & (0.11) \\
    0.3 & 0.40 & (0.01) & 0.32 & (0.14) & 0.19 & (0.00) \\
    0.4 & 0.43 & (0.05) & 0.25 & (0.04) & 0.21 & (0.05) \\
    0.5 & 0.44 & (0.06) & 0.43 & (0.24) & 0.19 & (0.00) \\
    0.6 & 0.45 & (0.06) & 0.32 & (0.17) & 0.24 & (0.06) \\
    0.7 & 0.48 & (0.07) & 0.41 & (0.22) & 0.24 & (0.06) \\
    0.8 & 0.44 & (0.07) & 0.40 & (0.21) & 0.29 & (0.20) \\
    0.9 & 0.44 & (0.06) & 0.23 & (0.00) & 0.28 & (0.19) \\
    1.0 & 0.46 & (0.05) & 0.23 & (0.00) & 0.34 & (0.24) \\

    \hline
    \end{tabular}
    \caption{Error rate results for $k = 3$ after MI feature selection.}
    \label{tbl:mi_errors_k3}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{charts/k3errors.pdf}
    \caption{Error rate results for $k = 3$ after MI feature selection.}
    \label{fig:mi_errors_k3}
\end{figure}

The results show slightly higher error rates, probably because the classifier is now overfitting the data. However it does show
the effect of the feature selection process much more clearly. The feature selection has a negative effect on \textit{bug}
despite it improving it when a higher value of $k$ is used. For \textit{suggestion} and \textit{question}, the accuracy appears
to be highest when ignoring any features lower than 0.3 - 0.4 bits but otherwise is generally worse when no features are ignored
or when nearly all the features are ignored. Unfortunately, the uncertainty is quite high as shown with the error bars in
Figure~\ref{fig:mi_errors_k3} which could be due to the training data being far too random.

\subsubsection{Conditional Mutual Information}
\label{sec:cmim}
Conditional mutual information (CMIM) extends the mutual information technique by taking into account pairs of features in order
to reduce redundancy.

Algorithm~\ref{alg:cmim}, adapted from \emph{``Fast Binary Feature Selection''} \cite{fast_binary_feature_selection}, shows a
na\"{i}ve approach to computing the best features using conditional mutual information.

\begin{algorithm}
    \caption{Na\"{i}ve CMIM}
    \label{alg:cmim}
    \begin{algorithmic}
        \FOR {$i \gets 1 \dots N$}
            \STATE $s[i] \gets I(l;f_i)$
        \ENDFOR
        \FOR {$j \gets 1 \dots N$}
            \STATE $nu[j] = \mathrm{argmax}_i s[i]$
            \FOR {$i \gets 1 \dots N$}
                \STATE $s[i] \gets \min(s[i], I(l;f_i | f_{nu[j]}))$
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

After running the algorithm, each feature had a calculated CMIM score. Some scores were set to `not a number' (NaN) as the
calculation failed due to some probabilities equating to 0 when calculating the entropy of the features. These features were
ignored which left 44, 55 and 44 features for \textit{bug}, \textit{suggestion} and \textit{question} respectively. The features
were then ranked linearly by their CMIM score. The new feature weights were then used for the k-NN classifier for different
values of $k$. The results can be seen in Figure~\ref{fig:cmim_errors}.

Although almost half the features were ignored, the error rates were almost identical to the error rates prior to feature
selection. One observation made was that the best value of $k$ for CMIM produced the lowest error rates for all three labels.
If this is a natural effect of the feature selection, then it is a good achievement as the same training parameters can be used
to achieve the best performance for all labels, therefore requiring only one training set to classify all three labels. It could
however be a coincidence, more experimentation would be required to prove this.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{charts/cmim_errors.pdf}
    \caption{Error rate results after CMIM feature selection.}
    \label{fig:cmim_errors}
\end{figure}

%--------------------------------------------------------------------------
% Analysis
%--------------------------------------------------------------------------
\section{Analysis}
Three feature selection techniques were investigated for this project. These were `trial and error', `mutual information' and
`conditional mutual information'. The trial and error technique achieved little improvement in accuracy but significantly
reduced the number of features included. As it did not take sets of features into account, feature redundancy could not be
determined. Interestingly, the number of included features was significantly lower for $k = 23$ than $k = 3$ as mutual
information and conditional mutual information did not test the classifier when computing their selection of features. The
filter methods would therefore always have a consistent feature selection and ranking across all values of $k$. Perhaps this is
a useful trait that the trial and error or other wrapper feature selection methods exhibit that improves the accuracy for a
particular model, not achieved using filter methods.

Figure~\ref{fig:inc_feature_comparison_bug} shows a comparison of the number of features that were included for each feature
selection technique for the \textit{bug} label. Each technique removed almost half the features and suggests that many of the
features were noise or were irrelevant. For this label, each technique removed a similar number of features reducing the
uncertainty, however each could be removing different sets of features. If further analysis is performed on all the techniques,
the features that were removed in all three techniques are more likely to be irrelevant to the classifier. The figure does
however show that conditional mutual information removes the least number of features, and that is the only technique that took
redundancy into account. This is an unexpected result as more features should have been removed due to them being redundant.

Mutual information and conditional mutual information reduced the number of features significantly as well, although neither
of them improved the accuracy. This can be seen by the comparison of Figure~\ref{fig:raw_errors}, Figure~\ref{fig:mi_prob} and
Figure~\ref{fig:cmim_errors}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{charts/included_feature_comparison.pdf}
    \caption{Comparison of the number of included features for \textit{bug}.}
    \label{fig:inc_feature_comparison_bug}
\end{figure}

%--------------------------------------------------------------------------
% Conclusions
%--------------------------------------------------------------------------
\section{Conclusions}
The aim of the project was to apply feature selection techniques to improve the accuracy of GitHub label classifiers. All the
features used were binary values representing whether a particular word or phrase was present in the body text of an issue. This
meant that the techniques used have only to cater for binary data. This also included the distance function used in the k-NN
classifier. The focus was primarily on feature selection, so only one classifier was tested. However
Section~\ref{sec:mutual_informaton} showed that just a different value of $k$ had a significant impact on the error rate where
issues labelled \textit{bug} performed well with a lower value of $k$ than issues labelled \textit{suggestion} or
\textit{question}.

Data was only exported from three different repositories, a total of 1797 examples and 93 features. It is likely some of these
were labelled incorrectly which could significantly affect the training data. The training data should include many more
repositories to improve the accuracy associated with any particular label, assuming that mislabelling is reasonably random.
Other features could also be examined, such as the user that submitted the issue or whether the body of the issue contained an
image.

The feature selection techniques, particularly the mutual information calculations showed that over half of the features were
irrelevant. These were mostly stop words that had not already been filtered out and non-stop words found in most of the issues.
These might have been filtered out during the text mining stage if tf-idf had been used, as mentioned in
Section~\ref{sec:background_wordfreq}.

The remaining features had a higher correlation with the labels, in either a positive or negative way as seen in
Figure~\ref{fig:mi_prob}. For example the \textit{bug} label, the features \textit{reproduce} and \textit{crash} had high
probability and high entropy meaning that if they are within the issue text, the issue is likely a bug. Whereas the features
\textit{nice}, \textit{thought} and \textit{cool} had a very low probability but a high entropy meaning that if they were in the
issue text, it is likely not a bug. This was promising as the nature of the words clearly match the results. Mutual information
in its simplest form therefore appears to provide a trustworthy measurement of correlation between a feature and a bug. It
however does not help reduce redundant features.

Unfortunately, neither of the feature selection techniques that were investigated were able to increase the accuracy by any
significant amount. Conditional mutual information in Section~\ref{sec:cmim} showed a slight hint of a more generalised solution
for achieving the best error rates across all labels using the same training parameters but otherwise showed no improvement
over the original error rates prior to any feature selection or ranking. Feature selection however does not guarantee any
improvement \cite{stability_feature_ranking}, particularly if it does not construct or evaluate a classification model. It was
also noted that in another paper comparing feature selection methods for spam filtering \cite{mlmethods_spamfiltering}, k-NN
proved to have lower accuracy than other classifiers such as na\"{i}ve Bayes or support vector machines (SVM). It also had a
very low precision, meaning that it was very sensitive to noise. This could be a possible explanation as to why the k-NN was
unable to show any improvement even after a variety of feature selection and ranking techniques. It may be possible for the k-NN
classifier to be improved by integrating the `l/k-rule' \cite{mlmethods_spamfiltering}, this adds a new parameter $l$ which can 
force a label to be classified as a particular example if $l$ or more neighbours are also of that label. Further work to this
project should include improving the k-NN classifier and the investigation of other classifiers to determine if they can be
individually improved using the same feature selection and ranking techniques used for the k-NN classifier.

%--------------------------------------------------------------------------
% Notes / ideas
%--------------------------------------------------------------------------
% - http://ats.cs.ut.ee/u/kt/hw/spam/spam.pdf page 67
%   http://airccse.org/journal/jcsit/0211ijcsit12.pdf page 176
%   l/k-rule (interesting?)
% 
% - http://computerresearch.org/stpr/index.php/gjcst/article/view/741/650
%   http://www.cs.rit.edu/~nan2563/feature_selection.pdf
%   http://www-nlp.stanford.edu/IR-book/
%   for feature selection techniques
% 
% 
% 
% 